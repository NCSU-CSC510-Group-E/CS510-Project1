% Please do not delete!  thanks! -- zach
% !TEX root = ../../main.tex

\section{Literature Review} \label{section:algorithms}
We propose two methods for analyzing and storing topics from books automatically: LDA and Doc2vec. 

\subsection{LDA}
\par In this section, we describe the Latent Dirichlet allocation(LDA) method for storing topics against books. 
\par LDA uses a Bayesian clustering approach to find topics relevant to the book. \cite{Genetics} The input to the method is a book (or later, a series of books) which consists of collection of words. 
\par This method assumes that each book (or books) consists of topics and that each topic has several key words associated with it. The number of topics associated with a book can be variable, and can be changed for better optimization. In this association, words are the measurable variable whereas the topics are "latent" variables, that is they are not directly measurable but indirectly observed. 
\par The performance of the method depends on the initial assumptions made. For example, some have assumed that books are random mixes of topics \cite{Paper}. The number of topics can initally be chosen as constant for a given book, or as a function of a chosen Poisson distribution. \cite{Paper} Topic distribution in the book is assumed to be a sparse Dirichlet function.

% This algorithm seems to be the clear winner.  

% Conceptually, this algorithm one-hot-encodes its input like Doc2vec does

% It then uses prior probabilities to identify words that are commonly paired with the given word.

% Through repeated training on the same data set, it will gradually identify topics among related text.

\subsection{Doc2Vec}
Doc2Vec is a model that extends an existing model, Word2Vec.  
Doc2vec mirrors Word2Vec in most ways except that it adds context information. \cite{RefWorks:doc:5a6e5748e4b0d609eec798dd}
% I'm not sure this algorithm really fits and shouldn't be replaced with something like LSI... see notes.  

% conceptually, this algorithm takes in a one-hot encoded vector and attempts to train a two-layer neural network to predict words around n-grams given.

% After iterations of training, it will give you a cluster of words that are similar to the given word.

% I guess we'd have to put something on top of Doc2Vec to use it for topic modeling.  I'm not entirely clear on how we'd do that. 

\subsection{Related Work}