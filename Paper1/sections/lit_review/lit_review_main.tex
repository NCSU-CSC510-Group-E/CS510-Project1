% Please do not delete!  thanks! -- zach
% !TEX root = ../../main.tex

\section{Literature Review} \label{section:algorithms}
We propose two methods for analyzing and storing topics from books automatically: LDA and Doc2vec. 

\subsection{LDA}
% This algorithm seems to be the clear winner.  

% Conceptually, this algorithm one-hot-encodes its input like Doc2vec does

% It then uses prior probabilities to identify words that are commonly paired with the given word.

% Through repeated training on the same data set, it will gradually identify topics among related text.

\subsection{Doc2Vec}
Doc2Vec is a model that extends an existing model, Word2Vec. Doc2vec mirrors Word2Vec in most ways except that it adds context information. \cite{RefWorks:doc:5a6e5748e4b0d609eec798dd}
% I'm not sure this algorithm really fits and shouldn't be replaced with something like LSI... see notes.  

% conceptually, this algorithm takes in a one-hot encoded vector and attempts to train a two-layer neural network to predict words around n-grams given.

% After iterations of training, it will give you a cluster of words that are similar to the given word.

% I guess we'd have to put something on top of Doc2Vec to use it for topic modeling.  I'm not entirely clear on how we'd do that.

% Not sure if the text below is 100% correct. Tried my best -M

	Doc2Vec is an implementation of the Paragraph Vector (PV) introduced by Mikolov and Le in 2014 - an unsupervised algorithm that generates vector representations of text. \cite{RefWorks:doc:5a6e5746e4b0d609eec798d7} The PV utilizes fixed length feature vectors learned from text sources of variable length. Mikolov offers two models of PV that are based on the implementation of Word2Vec: Distributed Memory Model (PV-DM) and Distributed Bag of Words (PV-DBOW).

%I would like to talk more about unsupervised algorithms and clustering but i don't quite understand it

PV-DM is similar to the Continuous Bag of Words (CBOW) model in Word2Vec such that is predicts the next word based on the given context. Paragraph vectors are concatenated with word vectors from that paragraph.  Each word vector is representing a word from the given context and a new vector is generated by concatenating all of those word vectors together to predict other words. These word vectors are able to remember context and semantics. This is an advantage over the traditional Bag of Words model which does not account for either resulting in each word having the same amount of weight to each other. Consequently similar words are not mapped close together. The paragraph vector allows the algorithm to remember the topic or 'label' of the paragraph, whether it is a sentence or a long document. 

PV-DBOW follows the skip-gram model of Word2Vec. In this model, instead of concatenating the paragraph vector with the word vectors formed in close proximity in order to predict the next word, random text from the paragraph is selected and a random word is selected from that text. Because this model does not save the word vectors and therefore does not retain as much information about the context and semantics of the text, it requires less storage.


\subsection{Related Work}
%Planning on talking about that twitter article at the very least, and a little about the recommendations made by Mikolv for Doc