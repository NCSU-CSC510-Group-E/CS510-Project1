% Please do not delete!  thanks! -- zach
% !TEX root = ../../main.tex

\section{Literature Review} \label{section:algorithms}
We propose two methods for analyzing and storing topics from books automatically: LDA and Doc2vec. 

\subsection{LDA}
\par In this section, we describe the Latent Dirichlet allocation(LDA) method for storing topics against books. 
\par LDA uses a Bayesian clustering approach to find topics relevant to the book. \ref{RefWorks:doc:5a721fb5e4b0d609eec83aa1} The input to the method is a book (or later, a series of books) which consists of collection of words. 
\par This method assumes that each book (or books) consists of topics and that each topic has several key words associated with it. The number of topics associated with a book can be variable, and can be changed for better optimization. In this association, words are the measurable variable whereas the topics are "latent" variables, that is they are not directly measurable but indirectly observed. 
\par The performance of the method depends on the initial assumptions made. For example, some have assumed that books are random mixes of topics \ref{RefWorks:doc:5a721e4ae4b095066af57410}. The number of topics can initally be chosen as constant for a given book, or as a function of a chosen Poisson distribution. \ref{RefWorks:doc:5a721e4ae4b095066af57410} Topic distribution in the book is assumed to be a sparse Dirichlet function.
\par A sparse Dirichlet function implies that we assume that a topic will be discussed only in a small breadth of pages in the book, and also that words relating to this topic will predominantly figure in these pages.
\par The algorithm looks to identify unique words to identify as topic names. Words like "the" and "a" are common and will occur with equal probability throughout the text. However, in say one chapter of the book some words may occur much more frequently in that particular chapter than the rest of the book, which means that the probability of these words in that chapter is more, which then the algorithm can use to choose a topic name and also a list of words related to that topic.

% This algorithm seems to be the clear winner.  

% Conceptually, this algorithm one-hot-encodes its input like Doc2vec does

% It then uses prior probabilities to identify words that are commonly paired with the given word.

% Through repeated training on the same data set, it will gradually identify topics among related text.

\subsection{Doc2Vec}
Doc2Vec is a model that extends an existing model, Word2Vec. Doc2vec mirrors Word2Vec in most ways except that it adds context information. \cite{RefWorks:doc:5a6e5748e4b0d609eec798dd}
% I'm not sure this algorithm really fits and shouldn't be replaced with something like LSI... see notes.  

% conceptually, this algorithm takes in a one-hot encoded vector and attempts to train a two-layer neural network to predict words around n-grams given.

% After iterations of training, it will give you a cluster of words that are similar to the given word.

% I guess we'd have to put something on top of Doc2Vec to use it for topic modeling.  I'm not entirely clear on how we'd do that.

% Not sure if the text below is 100% correct. Tried my best -M

	Doc2Vec is an implementation of the Paragraph Vector (PV) introduced by Mikolov and Le in 2014 - an unsupervised algorithm that generates vector representations of text. \cite{RefWorks:doc:5a6e5746e4b0d609eec798d7}  The basic difference between LDA and Doc2Vec is that LDA tries to form a structure of related words (a topic name with the list of associated words) out of books mainly on the basis of frequency of words in the document. While a simple yet powerful approach, it has a few drawbacks, such as semantics. For example, the words "powerful, strong and Paris" are equally distant when considered by a bag of words model, however "powerful" and "strong" are semantically close. \cite{RefWorks:doc:5a6e5746e4b0d609eec798d7}
	
	Paragraph Vector invloves predicting words in the paragraph. It is "unsupervised", which means that it predicts words in a paragraph and then uses these predictions to attempt to form structures of related words without any knowing of accuracy of the structure(s). The PV utilizes fixed length feature vectors learned from text sources of variable length.
	
	Mikolov offers two models of PV that are based on the implementation of Word2Vec: Distributed Memory Model (PV-DM) and Distributed Bag of Words (PV-DBOW).

%I would like to talk more about unsupervised algorithms and clustering but i don't quite understand it

PV-DM is similar to the Continuous Bag of Words (CBOW) model in Word2Vec such that is predicts the next word based on the given context. Paragraph vectors are concatenated with word vectors from that paragraph.  Each word vector is representing a word from the given context and a new vector is generated by concatenating all of those word vectors together to predict other words. These word vectors are able to remember context and semantics. The paragraph vector allows the algorithm to remember the topic or 'label' of the paragraph, whether it is a sentence or a long document. 
% This is an advantage over the traditional Bag of Words model which does not account for either resulting in each word having the same amount of weight to each other. Consequently similar words are not mapped close together. 

PV-DBOW follows the skip-gram model of Word2Vec. In this model, instead of concatenating the paragraph vector with the word vectors formed in close proximity in order to predict the next word, random text from the paragraph is selected and a random word is selected from that text. Because this model does not save the word vectors and therefore does not retain as much information about the context and semantics of the text, it requires less storage.


\subsection{Related Work}
%Planning on talking about that twitter article at the very least, and a little about the recommendations made by Mikolv for Doc
