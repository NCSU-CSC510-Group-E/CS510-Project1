% Please do not delete!  thanks! -- zach
% !TEX root = ../../main.tex

We propose two methods for analyzing and storing topics from books automatically: LDA and Doc2vec. 

\subsection{LDA}
\label{sec:lda}
\par In this section, we describe the Latent Dirichlet allocation(LDA) method for storing topics against books. 
\par LDA uses a Bayesian clustering approach to find topics relevant to the book. \cite{RefWorks:doc:5a721fb5e4b0d609eec83aa1} The input to the method is a book (or later, a series of books) which consists of collection of words. 
\par This method assumes that each book (or books) consists of topics and that each topic has several key words associated with it. The number of topics associated with a book can be variable, and can be changed for better optimization. In this association, the words comprising the text are thrown into a "bag-of-words" model such that grammer and word order are disregarded, but multiplicity is measurable. The calculated topics found from this bag-of-words are "latent" variables, that is they are not directly measurable but indirectly observed.  
\par The performance of the method depends on the initial assumptions made. For example, some have assumed that books are random mixes of topics \cite{RefWorks:doc:5a721e4ae4b095066af57410}. The number of topics can initally be chosen as constant for a given book, or as a function of a chosen Poisson distribution. \cite{RefWorks:doc:5a721e4ae4b095066af57410} Topic distribution in the book is assumed to be a sparse Dirichlet function.
\par A sparse Dirichlet function implies that we assume that a topic will be discussed only in a small breadth of pages in the book, and also that words relating to this topic will predominantly figure in these pages.
\par The algorithm looks to identify unique words to identify as topic names. Words like "the" and "a" are common and will occur with equal probability throughout the text. However, in say one chapter of the book some words may occur much more frequently in that particular chapter than the rest of the book, which means that the probability of these words in that chapter is more, which then the algorithm can use to choose a topic name and also a list of words related to that topic.

\subsection{Doc2Vec}
\label{sec:doc2vec}
Doc2Vec is a model that extends an existing model, Word2Vec. Doc2vec mirrors Word2Vec in most ways except that it adds context information. \cite{RefWorks:doc:5a6e5748e4b0d609eec798dd}

<<<<<<< HEAD
<<<<<<< HEAD
Doc2Vec is an implementation of the Paragraph Vector (PV) introduced by Mikolov and Le in 2014 - an unsupervised algorithm that generates vector representations of text. \cite{RefWorks:doc:5a6e5746e4b0d609eec798d7}  The basic difference between LDA and Doc2Vec is that LDA tries to form a structure of related words (a topic name with the list of associated words) out of books mainly on the basis of frequency of words in the document. While a simple yet powerful approach, it has a few drawbacks, such as semantics. For example, the words "powerful, strong and Paris" are equally distant when considered by a bag of words model, however "powerful" and "strong" are semantically close. \cite{RefWorks:doc:5a6e5746e4b0d609eec798d7}
	
Paragraph Vector involves predicting words in the paragraph. It is "unsupervised", which means that it predicts words in a paragraph and then uses these predictions to attempt to form structures of related words without any knowing of accuracy of the structure(s). The PV utilizes fixed length feature vectors learned from text sources of variable length. Mikolov offers two models of PV that are based on the implementation of Word2Vec: Distributed Memory Model (PV-DM) and Distributed Bag of Words (PV-DBOW). 
=======
\\subsubsection{Introduction}
=======
\subsubsection{Introduction}
>>>>>>> doc
Doc2Vec is an implementation of the Paragraph Vector (PV) introduced by Mikolov and Le in 2014 - an unsupervised algorithm that generates vector representations of text inspired from research of vector representations of words via neural networks. \cite{RefWorks:doc:5a6e5746e4b0d609eec798d7} By concatenationg or averaging vector represenations of words with other word vectors, the resulting vector can be used to predict future text. This technique offers one distinct advantage to bag-of-words models like LDA: the ability to record semantics due to the mapping of the word vectors into a vector space that allocates words of similar meaning together. For example, the words "powerful, strong and Paris" are equally distant when considered by a bag-of-words model, however "powerful" and "strong" are semantically close and should therefore be less distant from one another. \cite{RefWorks:doc:5a6e5746e4b0d609eec798d7}
	
PV is "unsupervised", which means that it predicts words in a paragraph and then uses these predictions to attempt to form structures of related words without any knowing of accuracy of the structure(s). The PV utilizes fixed length feature vectors learned from text sources of variable length. 


\subsubsection{Models}
Mikolov offers two models of PV that are based on the implementation of Word2Vec: Distributed Memory Model (PV-DM) and Distributed Bag of Words (PV-DBOW). 
>>>>>>> doc

PV-DM is similar to the Continuous Bag of Words (CBOW) model in Word2Vec such that is predicts the next word based on the given context. Paragraph vectors are concatenated with word vectors from that paragraph.  Each word vector is representing a word from the given context and a new vector is generated by concatenating all of those word vectors together to predict other words. These word vectors are able to remember context and semantics. The paragraph vector allows the algorithm to remember the topic or 'label' of the paragraph, whether it is a sentence or a long document. 

PV-DBOW follows the skip-gram model of Word2Vec. In this model, instead of concatenating the paragraph vector with the word vectors formed in close proximity in order to predict the next word, random text from the paragraph is selected and a random word is selected from that text. Because this model does not save the word vectors and therefore does not retain as much information about the context and semantics of the text, it requires less storage.

\subsubsection{Application}
-talk about different parameters and reults of gensim / Mikolov 

-talk about using all three models : one of each and one that uses both

Mikolov and Le recommend using a combination of PV-DM and PV-DBOW for consistently accurate results.  \cite{RefWorks:doc:5a6e5746e4b0d609eec798d7}

\subsection{Related Work}
\label{sec:related-work}

Doc2Vec was recently tested as the algorithm behind a recommender system with the goal of recommending unseen Twitter messages (tweets) relative to a user's typical activity and interests. A graph based link prediction method was used to infer which unseen tweets the users would like by "predicting the presence or absence of edges between nodes" on the graph.  \cite{RefWorks:doc:5a6e5746e4b0d609eec798d9} K values were tested in intervals divisible by 5 ranging from 5 to 35. The system preformed the best when K=30.

An LDA approach of a tagging system was implemented to improve tag recommendations. \cite{RefWorks:doc:5a73e055e4b0cf1dd767b18f} Tagging systems are often used for organizaing a user or organization's data. When this data is shared, tagging can be used to find or search for relative content.This system is similar to our proposed book recommendation system that uses subject topics to search for relative material. The initial tag data set was comprised of a large sample creating a diverse tag set. This set was used to elicit latent tags of sources that did not have that many tags to describe the document. Often, the tags recommended were more specifc. By increasing the number of tags and increasing specific tags, their approach contributed to the usefulness of searching for new content. 



