% Please do not delete!  thanks! -- zach
% !TEX root = ../../main.tex

We propose two methods for analyzing and storing topics from books automatically: LDA and Doc2vec. 

\subsection{LDA}
\label{sec:lda}
\input{sections/lit_review/01.lda.tex}

\subsection{Doc2Vec}
\label{sec:doc2vec}

\subsubsection{Introduction}

Doc2Vec is Gensim's implementation of the Paragraph Vector (PV) introduced by Mikolov and Le in 2014 - an unsupervised algorithm that generates vector representations of text inspired from research of vector representations of words via neural networks. \cite{RefWorks:doc:5aa699f3e4b0570b52523dee} \cite{RefWorks:doc:5a6e5746e4b0d609eec798d7} PV offers one distinct advantage to bag-of-words models like LDA: the ability to record semantics due to the mapping of the word vectors into a vector space that allocates words of similar meaning together. For example, the words "powerful, strong and Paris" are equally distant when considered by a bag-of-words model, however "powerful" and "strong" are semantically close and should therefore be less distant from one another. \cite{RefWorks:doc:5a6e5746e4b0d609eec798d7}
	
PV is "unsupervised", which means that it predicts words in a paragraph and then uses these predictions to attempt to form structures of related words without any knowing of accuracy of the structure(s). The PV utilizes fixed length feature vectors learned from text sources of variable length. 


\subsubsection{Models}
Mikolov offers two models of PV that are based on the implementation of Word2Vec: Distributed Memory Model (PV-DM) and Distributed Bag of Words (PV-DBOW). 

PV-DM is similar to the Continuous Bag of Words (CBOW) model in Word2Vec such that is predicts the next word based on the given context. Given a window of text, the vector representing the target word that is in the middle of the text window is formed by either concatenating or averaging all the word vectors around it, along with the paragraph vector for that document. These word vectors are able to remember context and semantics. The paragraph vector allows the algorithm to remember the topic or 'label' of the paragraph, whether it is a sentence or a long document. 

PV-DBOW follows the skip-gram model of Word2Vec. In this model, the word vectors are not saved, therefore it does not retain as much information about the context and semantics of the text. This makes DBOW faster and more resource efficient as it requires less memory. Instead of training word vectors, a single paragraph vector is trained for each document and it predicts words via the probability that the target word is in the document.

\subsubsection{Application}

Mikolov and Le recommend using a combination of PV-DM and PV-DBOW for consistently accurate results by combining the paragraph vectors in each model. \cite{RefWorks:doc:5a6e5746e4b0d609eec798d7} For simplicity, our evaluatoins use DM-M (mean), DM-C (concatenate), and DBOW only; not the combinations of DM and DBOW. Gensim's API makes it easy to change parameters that Mikolov and Gensim both determined to significantly affect results. \cite{RefWorks:doc:5aa698a1e4b0dae9a96dde2d} \cite{RefWorks:doc:5a6e5746e4b0d609eec798d7} Since we only have access to average computing resources and limited time to test the models, only a few of these parameters will be cross-validated in evaluations: window size (total window text size / 2 such that the target word is in the middle), feature fector size (the amount of 'neurons' in the word vectors), negative for DBOW (controls negative sampling), the epochs (number of iterations to train over the training corpus) and occasionally the minimum count of a word needed to be trained on throughout all the documents.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
