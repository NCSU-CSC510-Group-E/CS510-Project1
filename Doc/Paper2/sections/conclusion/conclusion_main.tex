% Please do not delete!  thanks! -- zach
% !TEX root = ../main.tex

\subsection{Process Conclusions}
\label{sec:conc:process-conclusions}
% Risk driven development worked fairly well.
This project is the first time anyone in the team applied the
principles of risk-driven development to their work, and the results
were fairly satisfying.
After the initial ambitions gave way to the true complexity of the
project, the team was unsure of how to proceed, and this process
provided a prescription to help dig out of the problem and into a
useful series of prototypes.
The prototypes themselves also served to illuminate the unknown risks
quite well.
For example, it was more or less assumed at the outset of the project
that third party libraries would make the implementation of learning
algorithms trivial and only a few days worth of work, but the process
of implementing and tweaking those algorithms actually consumed the
majority of the team's time.

% We met often enough by the end of the semester, but could have
% started that schedule earlier.
The group's meetings were a somewhat sensetive topic as well.
For the first two weeks of the project, the group met only once a
week, which was not often enough.
After that period, the team met twice a week for at least half an hour
(and most oftem more) to do some pair programming and to brainstorm
our solutions to problems encountered.
This proved to be closer to optimal and significant progress started
being made after that change.

% We used slack to great effect.
The team also made significant use of the chat app Slack.
Since the entire class was already using this tool, the team created a
private channel in the course's Slack server and used that as the
primary touchpoint for communication.
Random questions, brainstorming, research, and general coordination
was quite straightforward in Slack.

% We could have levereged github issues more.
% TODO: does this really add value?  Consider cutting
One more negative organizational conclusion was that the team could
have made significantly better use of an issue-tracking system.
Peer review revealed some platform specific issues and it would have
been easier to coordinate the response to those challenges if there
was one central repository to track progress. But since this is a 
relatively small project, the issues have been solved one by one cases. 


\subsection{LDA}
\label{sec:conc:lda}
% LDA is super dependent on the test set.
As expected, purely based on the accuracy (topics likelihood) of the 
topics, LDA is the clear winner in our battle of learning algorithms 
based purely on the measures presented. 
That said, it has some serious downsides.
% LDA can run unsupervised, but topic limiting is bad
While it should be considered a positive that the algorithm is capable
of learning topics unsupervised, the need to specify the number of
topics mined limits the flexibility.

Noticed that our independent Jaccard and Cosine measurements were not 
as good as topics likelihood, it could be the way that we calculated 
the measurements which only used certained labeled words, these words 
could be less similar to those words in the mined topics.

% LDA takes forever to run!
It is also worth noting that LDA can take significant portions of time
to train in a casual computing environment like a laptop.
There are some configurations of parameters that performed well in
evaluation but took upwards of 20 minutes simply to train on a few
thousand short Stack Overflow posts.
This can be mitigated somewhat with proper tuning and the algorithm
could be optomized for a high-performance environment, for instance, 
by refactoring the algorithms to run on distrbuted system, but it is
something that seems to be inherent in how to train the algorithm.

% 
\subsection{Doc2Vec}
\label{sec:conc:doc2vec}
Doc2Vec has some advantages over LDA that make it a desirable alternative. It can compare documents quickly if the model is already trained, eliminating LDA's wait time. In addition, there is no need to specify the number of topics to mine. However, Doc2Vec was unsuccessful at accurately generating topics as it could not distinguish between the different stackoverflow topics within an acceptable precision. If Doc2Vec has been able to distiguish between different topics, it could have been trained on pre-labeled documents such that an untrained, un-labeled document could have been suggested the labels from the most similar labeled document in the model. A better use for Doc2Vec might be as an extra feature of the Web Application to suggest text that is semantically similar to given text, i.e. finding a book author that writes in a similar syntax to the given author or text. 




\subsection{Future enhancement}
% TODO
The trained learning algorithms have not peroformed as well as
expected.
Doc2Vec’s classification of similar documents in different clusters combined with LDA’s relatively poor performance suggests strongly that the training data set has too much variation in it for the learning algorithms to correctly parse.  In other words, the training set, even after limiting to fewer topics, was probably too diverse to successfully mine topics from.
One approach the team considered but was unable to finish in time was
manually parsing Stack Overflow posts for posts that did include their
tags in the body and running the example on those.  

The next obvious goal would be to start building the rest of the
system.
Once the problem of topic modeling is more appropriately solved, the
suggestion algorithm discussed in \ref{sec:system-design}.
Once Equation \ref{sec:system-design:eq:priority} is implemented in a REST API, the
actual webapp could easily be built quickly.  

\section{Chits}
\begin{itemize}
\item FNL
\item VVR
\item AFJ
\item GVO
\item YLV
\item RBC
\item VZV
\item WLB
\item XYU
\item BLM
\end{itemize}

% Can we make a chart of Doc2Vec vs LDA?  Looks like LDA is the winner

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
 